{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.tasks.python.vision import PoseLandmarker, PoseLandmarkerOptions\n",
    "from mediapipe.tasks.python.core.base_options import BaseOptions\n",
    "import definitions as defs\n",
    "\n",
    "def load_blazepose(model_path = 'models/blazepose.task'):\n",
    "    options = PoseLandmarkerOptions(\n",
    "        base_options=BaseOptions(model_asset_path=model_path),\n",
    "        running_mode=vision.RunningMode.IMAGE)\n",
    "    model = PoseLandmarker.create_from_options(options)\n",
    "    return model\n",
    "\n",
    "def load_posenet():\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"models/posenet2.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "def load_movenet():\n",
    "    interpreter = tf.lite.Interpreter(model_path='models/movenet-lightning.tflite')\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "def blazepose(model, img):\n",
    "    # TODO: check that this doesn't cause distortion.\n",
    "    mp_img = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)\n",
    "    output = model.detect(mp_img)\n",
    "    # print(output.pose_landmarks)\n",
    "    # TODO: convert to standard form\n",
    "    return output.pose_landmarks\n",
    "\n",
    "def movenet(interpreter, img):\n",
    "    output = run_inference_tflite(interpreter, img, dtype = defs.MOVENET_DTYPE, shape=defs.MOVENET_SHAPE)\n",
    "    return output\n",
    "\n",
    "def posenet(interpreter, img):    \n",
    "    map = run_inference_tflite(interpreter, img, dtype = defs.POSENET_DTYPE, shape=defs.POSENET_SHAPE)\n",
    "    return map\n",
    "\n",
    "def run_inference_tflite(interpreter, img, dtype, shape=257):\n",
    "    # TODO: check how to perform this resize without distortion. this will not yield accurate kp\n",
    "    # positions, probably.\n",
    "    input_image = cv.resize(img, (shape, shape))\n",
    "    cv.waitKey(0)\n",
    "    input_image = tf.cast(input_image, dtype=dtype)\n",
    "    input_image = tf.expand_dims(input_image, axis=0)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    interpreter.invoke()\n",
    "    kps = np.squeeze(interpreter.get_tensor(output_details[0]['index']))\n",
    "    # TODO: convert to standard form\n",
    "    return kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "model = load_posenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_posenet(map, offset, threshold=0):\n",
    "    print(offset.shape)\n",
    "    keypoints = []\n",
    "    for idx in range(map.shape[-1]):\n",
    "        cur_map = map[..., idx]\n",
    "        y, x = np.unravel_index(np.argmax(cur_map), cur_map.shape)\n",
    "        cur_offset_y = offset[y, x, idx * 2]\n",
    "        cur_offset_x = offset[y, x, idx * 2 + 1]\n",
    "\n",
    "        conf = map[y, x, idx]\n",
    "        conf = 1 / (1 + np.exp(-conf))\n",
    "\n",
    "        y = y / 8 * defs.POSENET_SHAPE + cur_offset_y\n",
    "        x = x / 8 * defs.POSENET_SHAPE + cur_offset_x\n",
    "        y = max(0, min(y, defs.POSENET_SHAPE - 1))\n",
    "        x = max(0, min(x, defs.POSENET_SHAPE - 1))\n",
    "\n",
    "        print('here')\n",
    "        \n",
    "        if conf > threshold:\n",
    "            kp = defs.KP2D(idx, y, x, conf, defs.KP_DICT_17[idx])\n",
    "            print(f'{kp.name} {kp.prob} {kp.x} {kp.y}')\n",
    "            keypoints.append(kp)\n",
    "    return keypoints\n",
    "\n",
    "im = cv.imread('data/raw/test.jpg', cv.IMREAD_COLOR)\n",
    "map = posenet(model, im)\n",
    "\n",
    "out = model.get_output_details()\n",
    "offset = np.squeeze(model.get_tensor(out[1]['index']))\n",
    "proc = process_posenet(map, offset=offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9, 9, 34)\n",
      "post squeeze\n",
      "(9, 9, 34)\n"
     ]
    }
   ],
   "source": [
    "out = model.get_output_details()\n",
    "print(offset.shape)\n",
    "print('post squeeze')\n",
    "offset = np.squeeze(offset)\n",
    "print(offset.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
