{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe.tasks.python.vision import PoseLandmarker, PoseLandmarkerOptions\n",
    "from mediapipe.tasks.python.core.base_options import BaseOptions\n",
    "import definitions as defs\n",
    "\n",
    "def load_blazepose(model_path = 'models/blazepose.task'):\n",
    "    options = PoseLandmarkerOptions(\n",
    "        base_options=BaseOptions(model_asset_path=model_path),\n",
    "        running_mode=vision.RunningMode.IMAGE)\n",
    "    model = PoseLandmarker.create_from_options(options)\n",
    "    return model\n",
    "\n",
    "def load_posenet():\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"models/posenet2.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "def load_movenet():\n",
    "    interpreter = tf.lite.Interpreter(model_path='models/movenet-lightning.tflite')\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "def blazepose(model, img):\n",
    "    # TODO: check that this doesn't cause distortion.\n",
    "    mp_img = mp.Image(image_format=mp.ImageFormat.SRGB, data=img)\n",
    "    output = model.detect(mp_img)\n",
    "    # print(output.pose_landmarks)\n",
    "    # TODO: convert to standard form\n",
    "    return output.pose_landmarks\n",
    "\n",
    "def movenet(interpreter, img):\n",
    "    output = run_inference_tflite(interpreter, img, dtype = defs.MOVENET_DTYPE, shape=defs.MOVENET_SHAPE)\n",
    "    return output\n",
    "\n",
    "def posenet(interpreter, img):    \n",
    "    map = run_inference_tflite(interpreter, img, dtype = defs.POSENET_DTYPE, shape=defs.POSENET_SHAPE)\n",
    "    return map\n",
    "\n",
    "def run_inference_tflite(interpreter, img, dtype, shape=257):\n",
    "    # TODO: check how to perform this resize without distortion. this will not yield accurate kp\n",
    "    # positions, probably.\n",
    "    input_image = cv.resize(img, (shape, shape))\n",
    "    input_image = tf.cast(input_image, dtype=dtype)\n",
    "    input_image = tf.expand_dims(input_image, axis=0)\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n",
    "    interpreter.invoke()\n",
    "    kps = np.squeeze(interpreter.get_tensor(output_details[0]['index']))\n",
    "    # TODO: convert to standard form\n",
    "    return kps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_posenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 9, 34)\n",
      "nose 0.00025723957319332536 166.05007934570312 37.550079345703125 5.425079345703125\n",
      "left_eye 0.00036297483040718844 123.02731370925903 0 -5.472686290740967\n",
      "right_eye 0.00030371665845830876 123.93878412246704 220.31378412246704 -4.561215877532959\n",
      "left_ear 0.0005375043107950707 256.0625375509262 0 -0.9374624490737915\n",
      "right_ear 0.00041713279253397477 131.4853491783142 35.11034917831421 2.985349178314209\n",
      "left_shoulder 0.0008302593871404658 256.4144524335861 256.4144524335861 -0.5855475664138794\n",
      "right_shoulder 0.0006376022171620848 248.3817138671875 0 -8.6182861328125\n",
      "left_elbow 0.0007801340093478948 256.21784871816635 0 -0.7821512818336487\n",
      "right_elbow 0.001244436626206526 259.904732465744 2.9047324657440186 2.9047324657440186\n",
      "left_wrist 0.0015133987144239534 194.37537479400635 226.50037479400635 1.6253747940063477\n",
      "right_wrist 0.000602735146169159 257.2635175585747 0.2635175585746765 0.2635175585746765\n",
      "left_hip 0.001018136171984477 253.95867204666138 0 -3.041327953338623\n",
      "right_hip 0.0010702095759588498 252.147807598114 0 -4.852192401885986\n",
      "left_knee 0.002529309789736458 192.26371908187866 192.26371908187866 -0.4862809181213379\n",
      "right_knee 0.0015386077478065636 252.43003940582275 0 -4.569960594177246\n",
      "left_ankle 0.003644468161564452 195.79134798049927 163.66634798049927 3.0413479804992676\n",
      "right_ankle 0.003377377978460764 193.14075377583504 225.26575377583504 0.39075377583503723\n"
     ]
    }
   ],
   "source": [
    "def process_posenet(map, offset, threshold=0):\n",
    "    print(offset.shape)\n",
    "    keypoints = []\n",
    "    for idx in range(map.shape[-1]):\n",
    "        cur_map = map[..., idx]\n",
    "        y, x = np.unravel_index(np.argmax(cur_map), cur_map.shape)\n",
    "        cur_offset = offset[x, y, idx]\n",
    "        conf = map[y, x, idx]\n",
    "        conf = 1 / (1 + np.exp(-conf))\n",
    "\n",
    "        y = y / 8 * defs.POSENET_SHAPE + cur_offset\n",
    "        x = x / 8 * defs.POSENET_SHAPE + cur_offset\n",
    "        if(y < 0): y = 0\n",
    "        if(x < 0): x = 0\n",
    "                \n",
    "        if conf > threshold:\n",
    "            kp = defs.KP2D(idx, y, x, conf, defs.KP_DICT_17[idx])\n",
    "            print(f'{kp.name} {kp.prob} {kp.x} {kp.y} {cur_offset}')\n",
    "            keypoints.append(kp)\n",
    "    return keypoints\n",
    "\n",
    "im = cv.imread('data/raw/test.jpg', cv.IMREAD_COLOR)\n",
    "map = posenet(model, im)\n",
    "\n",
    "out = model.get_output_details()\n",
    "offset = np.squeeze(model.get_tensor(out[1]['index']))\n",
    "proc = process_posenet(map, offset=offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9, 9, 34)\n",
      "post squeeze\n",
      "(9, 9, 34)\n"
     ]
    }
   ],
   "source": [
    "out = model.get_output_details()\n",
    "print(offset.shape)\n",
    "print('post squeeze')\n",
    "offset = np.squeeze(offset)\n",
    "print(offset.shape)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
